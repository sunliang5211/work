{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'/home/hb_liulian/data/train/hbsubs/train_colrename.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-4a2126c497c0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mmodel_savepath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'/home/hb_liulian/Model/Model_path/hbsubs/'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0mresult_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'/home/hb_liulian/result_path/'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_savepath\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'train_colrename.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32md:\\program files (x86)\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    707\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    708\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 709\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    710\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    711\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\program files (x86)\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    447\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 449\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\program files (x86)\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    816\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 818\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    819\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    820\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\program files (x86)\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1047\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1048\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1049\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1050\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1051\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\program files (x86)\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1693\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'allow_leading_cols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1694\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1695\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1696\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1697\u001b[0m         \u001b[1;31m# XXX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: File b'/home/hb_liulian/data/train/hbsubs/train_colrename.csv' does not exist"
     ]
    }
   ],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "#导入相关包\n",
    "import os,sys\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import log_loss\n",
    "from imblearn.over_sampling import SMOTE\n",
    "#设置路径,读取数据\n",
    "train_savepath = '/home/hb_liulian/data/train/hbsubs/'\n",
    "test_savepath = '/home/hb_liulian/data/test/hbsubs/'\n",
    "model_savepath = '/home/hb_liulian/Model/Model_path/hbsubs/'\n",
    "result_path = '/home/hb_liulian/result_path/'\n",
    "df = pd.read_csv(train_savepath + 'train_colrename.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup_feaim0():\n",
    "    feaim_list = list(base_model.feature_importance())\n",
    "    feaim0_list = []\n",
    "    for i in range(len(feaim_list)):\n",
    "        if list(base_model.feature_importance())[i] ==0:\n",
    "            feaim0_list.append(i)\n",
    "        else:\n",
    "            pass\n",
    "    return feaim0_list\n",
    "\n",
    "def str_indexlookup(df):\n",
    "    list1 = []\n",
    "    for i in range(df.shape[1]):\n",
    "        if list(df.dtypes != object)[i]:\n",
    "            pass\n",
    "        else:\n",
    "            list1.append(i)\n",
    "    use_col = list(df.columns[list(set(list(range(df.shape[1])))-set(list1))])\n",
    "    return df.loc[:,use_col]\n",
    "\n",
    "def lgb_cv_fit(alg,useTrainCV=True):\n",
    "    if useTrainCV:\n",
    "        cv_params = alg.get_params()\n",
    "        global cv_results\n",
    "        cv_results = lgb.cv(\n",
    "            cv_params, lgb_train, num_boost_round=alg.get_params()['num_iterations'], nfold=5, \n",
    "            stratified=True, shuffle=True, metrics='binary_logloss',\n",
    "            early_stopping_rounds=50, verbose_eval=50, show_stdv=True, seed=0)\n",
    "        num_iterations = len(cv_results['binary_logloss-mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_list = [\n",
    "'CITY_CODE',\n",
    "'AREA_ID',\n",
    "'USER_TYPE',\n",
    "'SERVICE_TYPE',\n",
    "'PAY_MODE',\n",
    "'INNET_METHOD',\n",
    "'BRAND_ID',\n",
    "'USER_STAR_LVL',\n",
    "'USER_STATUS_TYPE',\n",
    "'GENDER',\n",
    "'AGE',\n",
    "'CREDENTIALS_TYPE',\n",
    "'ONECARD_MSISDN_CNT',\n",
    "'IS_PRIME_GOTONE',\n",
    "'IS_BROAD_BIND',\n",
    "'IS_UNLIM_PLAN',\n",
    "'IS_CM_UNLIM_EFF',\n",
    "'MAIN_ASSI_USER',\n",
    "'ASSI_MAIN_USER_ID',\n",
    "'IS_MAIN_ASSI',\n",
    "'ORD_BUSI_TYPE',\n",
    "'ASSI_TYPE',\n",
    "'IS_THIS_DEV',\n",
    "'ID_BREAK_TYPE',\n",
    "'IS_SAKA',\n",
    "'IS_DM_SEEP',\n",
    "'IS_BROAD_SEEP',\n",
    "'IS_THIS_HALT',\n",
    "'IS_GROUP',\n",
    "'IS_GROUP_IMP',\n",
    "'MEM_TYPE',\n",
    "'IS_SCHOOL_USER',\n",
    "'IS_SCHOOL_AREA_USER',\n",
    "'IS_FAM_VNET',\n",
    "'IS_DEV_SHAM',\n",
    "'IS_FLEA',\n",
    "'IS_FLUX_SHAM',\n",
    "'IS_LOW_STATUS_EXCEPT',\n",
    "'IS_LOW_ACTIVE',\n",
    "'IS_THIS_CHANGE_PLAN',\n",
    "'TERM_OS',\n",
    "'IS_DOUBLE_CARD',\n",
    "'DOUBLE_CARD_TYPE',\n",
    "'IS_RAISE_CARD',\n",
    "'RAISE_CARD_TYPE',\n",
    "'ONE_IMEI_SHARE_TYPE',\n",
    "'IS_DOUBLE_IMEI',\n",
    "'IMSI1_USER_TYPE',\n",
    "'IMSI2_USER_TYPE',\n",
    "'BASE_PLAN_PRICE',\n",
    "'IS_4G_USER',\n",
    "'IS_4G_OPEN',\n",
    "'TERM_4G_CUST_FLAG',\n",
    "'MOBILE_4G_CUST_FLAG',\n",
    "'MIFI_4G_CUST_FLAG',\n",
    "'CPE_4G_CUST_FLAG',\n",
    "'CARD_4G_CUST_FLAG',\n",
    "'USE_NET_4G_CUST_FLAG',\n",
    "'IS_FAV_FEE',\n",
    "'IS_THIS_ACCT',\n",
    "'IS_THIE_OWE',\n",
    "'IS_THIS_ACCT_RETAIN',\n",
    "'IS_VOICE_FLAG',\n",
    "'IS_TOLL_VOICE_FLAG',\n",
    "'IS_ROAM_VOICE_FLAG',\n",
    "'IS_THIS_ACTIVE',\n",
    "'IS_THIS_W3',\n",
    "'IS_THIS_SILENT',\n",
    "'IS_THIS_NOMINAL',\n",
    "'IS_MKT_CASE_USER',\n",
    "'LAST_MKT_CASE_KIND',\n",
    "'IS_DOUBLE_REDUCE',\n",
    "'is_diff_net_user',\n",
    "'two_imei_user_cnt'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = str_indexlookup(df)\n",
    "df = df.dropna(axis = 1,thresh = df.shape[0]*0.6)\n",
    "df = df.fillna(method='ffill')\n",
    "#label转换\n",
    "df['LABEL'].loc[df['LABEL']==True] = 'a'\n",
    "df['LABEL'].loc[df['LABEL']==False] = 'b'\n",
    "df['LABEL'].loc[df['LABEL']=='a'] = 0\n",
    "df['LABEL'].loc[df['LABEL']=='b'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:,1:df.shape[1]-1]# 设置训练集特征\n",
    "y = df.iloc[:,df.shape[1]-1]  # 设置训练集标签\n",
    "#划分测试机训练集\n",
    "l_train, l_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#对训练集数据用SMOTE算法合成pos_sample\n",
    "smo = SMOTE(ratio={1: round(y_train.shape[0]/4)},random_state=42)\n",
    "X_smo, y_smo = smo.fit_sample(l_train, y_train)\n",
    "df_xsmo = pd.DataFrame(X_smo)\n",
    "df_ysmo = pd.DataFrame(y_smo)\n",
    "df_xsmo_renamedict = dict(zip(df_xsmo.columns.values.tolist(),X.columns.values.tolist()))\n",
    "df_xsmo.rename(columns = df_xsmo_renamedict,inplace=True)\n",
    "df_ysmo.rename(columns = {0:'label'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_smo' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-9a440edc05c5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mcv_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#划分测试集训练集\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmcw\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mpow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_smo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mlgb_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_smo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_smo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msilent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#计算scale_pos_weight\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_smo' is not defined"
     ]
    }
   ],
   "source": [
    "cv_results = 1\n",
    "#划分测试集训练集\n",
    "mcw=1/pow(y_smo.sum(),0.5)\n",
    "lgb_train = lgb.Dataset(df_xsmo, label = df_ysmo, silent=True)\n",
    "                        #feature_name = df_xsmo.columns.values.tolist(),\n",
    "                        #categorical_feature = cat_list)\n",
    "#计算scale_pos_weight\n",
    "scale_pos_weight = y_smo.sum()/y_smo.shape[0]\n",
    "#不平衡样本分层抽样\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=3)\n",
    "#设置lgb模型\n",
    "bst = lgb.LGBMClassifier(objective='binary'\n",
    "                            ,num_leaves=32\n",
    "                            ,learning_rate=0.05\n",
    "                            ,num_iterations=2000\n",
    "                            ,max_depth=5\n",
    "                            ,metric='auc'\n",
    "                            ,feature_fraction=0.8\n",
    "                            ,bagging_fraction = 0.8\n",
    "                            ,bagging_freq = 5\n",
    "                            ,pos_bagging_fraction = 1\n",
    "                            ,neg_bagging_fraction = scale_pos_weight\n",
    "                            ,n_jobs=-1\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('-------------------开始第一次迭代轮数计算-------------------')\n",
    "begintime=time.perf_counter()\n",
    "lgb_cv_fit(bst, X_smo, y_smo)\n",
    "endtime=time.perf_counter()\n",
    "print('best_num_iterations:',len(cv_results['auc-mean']))\n",
    "print('best_cv_score:', cv_results['auc-mean'][-1])\n",
    "print ('耗时：',endtime-begintime)\n",
    "print('-------------------第一次迭代轮数计算结束-------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#挑选无用特征\n",
    "param = bst.get_params()\n",
    "lgb_train = lgb.Dataset(df_xsmo, label = df_ysmo, silent=True)\n",
    "                        #feature_name = df_xsmo.columns.values.tolist(),\n",
    "                        #categorical_feature = cat_list)\n",
    "base_model=lgb.train(param,lgb_train)\n",
    "\n",
    "base_model.feature_importance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#挑选无用特征\n",
    "param = bst.get_params()\n",
    "lgb_train = lgb.Dataset(df_xsmo, label = df_ysmo, silent=True)\n",
    "                        #feature_name = df_xsmo.columns.values.tolist(),\n",
    "                        #categorical_feature = cat_list)\n",
    "base_model=lgb.train(param,lgb_train)\n",
    "use_fea = list(set(list(range(l_train.shape[1])))-set(lookup_feaim0()))\n",
    "X_refea = X.iloc[:,use_fea]# 设置剔除无用特征训练集特征\n",
    "y_refea = df.iloc[:,df.shape[1]-1] # 设置训练集标签\n",
    "#对训练集数据用SMOTE算法合成pos_sample\n",
    "smo = SMOTE(ratio={1: round(y_train.shape[0]/4)},random_state=42)\n",
    "X_smo_refea, y_smo_refea = smo.fit_sample(X_refea, y_refea)\n",
    "df_xsmo_refea = pd.DataFrame(X_smo_refea)\n",
    "df_ysmo_refea = pd.DataFrame(y_smo_refea)\n",
    "df_xsmo_refea_renamedict = dict(zip(df_xsmo_refea.columns.values.tolist(),X_refea.columns.values.tolist()))\n",
    "df_xsmo_refea.rename(columns = df_xsmo_refea_renamedict,inplace=True)\n",
    "df_ysmo_refea.rename(columns = {0:'label'},inplace=True)\n",
    "cat_refea_list = list(set(cat_list) - set(X.iloc[:,lookup_feaim0()].columns.values.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_train = lgb.Dataset(df_xsmo_refea, label = df_ysmo_refea, \n",
    "                        silent=True,\n",
    "                        feature_name = df_xsmo_refea.columns.values.tolist(),\n",
    "                        categorical_feature = cat_refea_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('-------------------开始第二次迭代轮数计算-------------------')\n",
    "begintime=time.perf_counter()\n",
    "lgb_cv_fit(bst)\n",
    "endtime=time.perf_counter()\n",
    "print('best_n_estimators:',len(cv_results['auc-mean']))\n",
    "print('best_cv_score:', cv_results['auc-mean'][-1])\n",
    "print ('耗时：',endtime-begintime)\n",
    "print('-------------------第二次迭代轮数计算结束-------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_test1={\n",
    "    'n_estimators':[len(cv_results['auc-mean'])]\n",
    "    ,'learning_rate':np.arange(0.01,0.2,0.01)\n",
    "    ,'min_child_weight':[mcw]\n",
    "    #,'min_child_samples': [18,19,20,21,22]\n",
    "    ,'max_depth': np.arange(3,8,1)\n",
    "    ,'num_leaves':np.arange(6,260,20)\n",
    "    ,'bagging_fraction': [0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "    ,'bagging_freq':[5]\n",
    "    ,'pos_bagging_fraction':[1]\n",
    "    ,'neg_bagging_fraction':[scale_pos_weight]\n",
    "    ,'reg_alpha':np.arange(0,4,0.5)\n",
    "    ,'reg_lambda':np.arange(0,4,0.5)\n",
    "}\n",
    "clf = RandomizedSearchCV(estimator=bst, \n",
    "                              param_distributions=params_test1, \n",
    "                              scoring='f1', \n",
    "                              cv=5, \n",
    "                              verbose=1\n",
    "                              )\n",
    "#clf = BayesianOptimization(bst,params_test1,random_state=7)\n",
    "print('-------------------开始搜索最佳超参数-------------------')\n",
    "begintime=time.perf_counter()\n",
    "clf.fit(df_xsmo_refea, df_ysmo_refea)\n",
    "#clf.maximize(init_points=10, n_iter=30, acq='ei', xi=0.0)\n",
    "endtime=time.perf_counter()\n",
    "clf.score, clf.best_params_, clf.best_score_#根据设置的测试参数，计算出最优模型参数\n",
    "print ('耗时：',endtime-begintime)\n",
    "print('-------------------最佳超参数搜索结束-------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_train_refea = lgb.Dataset(df_xsmo_refea, label = df_ysmo_refea, \n",
    "                        silent=True,\n",
    "                        feature_name = df_xsmo_refea.columns.values.tolist(),\n",
    "                        categorical_feature = cat_refea_list)\n",
    "\n",
    "l_test = l_test.iloc[:,use_fea]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = clf.best_params_\n",
    "savemodel=lgb.train(param,lgb_train)\n",
    "savemodel.save_model(model_savepath+'lgb_subs_model_smo')\n",
    "bst = lgb.Booster(model_file=model_savepath+'lgb_subs_model_smo')\n",
    "ypred = bst.predict(l_test, num_iteration=bst.best_iteration)\n",
    "df_ypred = pd.DataFrame(ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#对预测结果分层\n",
    "df1 = pd.DataFrame(ypred)\n",
    "estimator = KMeans(n_clusters=5)\n",
    "estimator.fit(df1)\n",
    "label_pred = estimator.labels_ #获取聚类标签\n",
    "centroids = estimator.cluster_centers_ #获取聚类中心\n",
    "df1['Kmeans'] = label_pred\n",
    "df = pd.DataFrame(df1)\n",
    "#根据预测概率，从高到低改变聚类标签\n",
    "for i in range(5):\n",
    "    df['Kmeans'].ix[df['Kmeans'] ==i]=str(sorted(centroids).index(centroids[i]))+\"星\"\n",
    "#计算各个阈值的pre,re,f1\n",
    "#df['label'] = y_test_smo\n",
    "df['label'] = y_test.values\n",
    "df['count'] = 1\n",
    "df_res=pd.pivot_table(df, index=['Kmeans'],\n",
    "    values = ['count','label'],\n",
    "    aggfunc = {'count': np.sum,'label':np.sum})\n",
    "df_res['precision'] = ''\n",
    "df_res['recall'] = ''\n",
    "df_res['实际订购率'] = df_res['label']/df_res['count']\n",
    "count_list = list(df_res['count'].values)\n",
    "label_list = list(df_res['label'].values)\n",
    "for i in range(5):\n",
    "    df_res['precision'][i] = sum(label_list[i:5])/sum(count_list[i:5])\n",
    "    df_res['recall'][i] = sum(label_list[i:5])/sum(label_list[0:5])\n",
    "df_res['f1'] = 2*df_res['precision']*df_res['recall']/(df_res['precision']+df_res['recall'])\n",
    "df_res['precision'] =df_res['precision'].apply(lambda x: '%.4f%%' % (x*100))\n",
    "df_res['recall'] =df_res['recall'].apply(lambda x: '%.4f%%' % (x*100))\n",
    "df_res['f1'] =df_res['f1'].apply(lambda x: '%.4f%%' % (x*100))\n",
    "df_res['实际订购率'] =df_res['实际订购率'].apply(lambda x: '%.4f%%' % (x*100))\n",
    "print(df_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res.to_csv(result_path+'res_smote.csv')\n",
    "df_ypred.to_csv(result_path+'res_pre.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
